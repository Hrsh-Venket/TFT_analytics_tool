{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFT Ranking Model Training\n",
    "\n",
    "Train a deep learning model to predict TFT match placements using multi-head self-attention.\n",
    "\n",
    "**Architecture**: Input → FC → 3×Multi-Head Self-Attention (2 heads) → FC → Scores\n",
    "\n",
    "**Loss**: LambdaNDCG Loss (optimized for ranking)\n",
    "\n",
    "**Metrics**: NDCG@8, Per-rank Accuracy, MAE, Top-4 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install dependencies and upload data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install pytorchltr h5py tensorboard tqdm psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data file (for Colab)\n",
    "# Uncomment the following lines if running on Colab\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload tft_data_splits.h5\n",
    "\n",
    "# If running locally or data is already available, just set the path:\n",
    "DATA_PATH = \"tft_data_splits.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (for Colab)\n",
    "# Uncomment if running on Colab and need to clone the repo\n",
    "\n",
    "# !git clone https://github.com/YOUR_USERNAME/TFT_analytics_tool.git\n",
    "# %cd TFT_analytics_tool\n",
    "\n",
    "# If running locally, just navigate to the project directory:\n",
    "import os\n",
    "# os.chdir('/path/to/TFT_analytics_tool')  # Adjust if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Adjust hyperparameters here. All settings are easily configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.model.config import Config\n",
    "\n",
    "# ============================================================================\n",
    "# ADJUST THESE SETTINGS AS NEEDED\n",
    "# ============================================================================\n",
    "\n",
    "# Model Architecture\n",
    "Config.N_ATTENTION_LAYERS = 3      # Number of attention layers\n",
    "Config.N_HEADS = 2                  # Number of attention heads\n",
    "Config.HIDDEN_DIM = 256             # Hidden dimension\n",
    "Config.DROPOUT = 0.2                # Dropout probability\n",
    "\n",
    "# Training Hyperparameters\n",
    "Config.BATCH_SIZE = 64              # Batch size\n",
    "Config.LEARNING_RATE = 1e-4         # Learning rate\n",
    "Config.EPOCHS = 50                  # Maximum epochs\n",
    "Config.EARLY_STOPPING_PATIENCE = 10 # Early stopping patience\n",
    "\n",
    "# Loss Function\n",
    "Config.LAMBDA_NDCG_SIGMA = 1.0      # LambdaNDCG sigma parameter\n",
    "\n",
    "# Data\n",
    "Config.NORMALIZE_FEATURES = True    # Normalize input features\n",
    "Config.NUM_WORKERS = 2              # DataLoader workers (set to 0 if issues)\n",
    "\n",
    "# Saving\n",
    "Config.MODEL_SAVE_DIR = \"saved_models\"\n",
    "Config.SAVE_BEST_MODEL = True\n",
    "Config.SAVE_EVERY_EPOCH = False\n",
    "\n",
    "# Random Seed\n",
    "Config.RANDOM_SEED = 42\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Print configuration\n",
    "Config.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Warning: No GPU available. Training will be slow on CPU.\")\n",
    "\n",
    "print(f\"\\nDevice: {Config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model\n",
    "\n",
    "This will:\n",
    "- Load data from HDF5\n",
    "- Create the model\n",
    "- Train with LambdaNDCG loss\n",
    "- Validate after each epoch\n",
    "- Save the best model\n",
    "- Log metrics to TensorBoard\n",
    "- Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.model.train import train_model\n",
    "\n",
    "# Train model\n",
    "model, test_metrics = train_model(\n",
    "    hdf5_path=DATA_PATH,\n",
    "    config=Config,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View TensorBoard (Optional)\n",
    "\n",
    "Visualize training progress with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (for Jupyter/Colab)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Saved Model (Optional)\n",
    "\n",
    "Load and evaluate a previously saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.model.evaluate import evaluate_saved_model\n",
    "\n",
    "# Evaluate best model on test set\n",
    "metrics = evaluate_saved_model(\n",
    "    model_path=\"saved_models/best_model.pt\",\n",
    "    hdf5_path=DATA_PATH,\n",
    "    split=\"test\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions (Optional)\n",
    "\n",
    "Use the trained model to make predictions on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.model.evaluate import load_model\n",
    "from ml.model.data_utils import load_and_prepare_data\n",
    "from ml.model.metrics import scores_to_placements, relevance_to_placements\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"saved_models/best_model.pt\", device=Config.DEVICE)\n",
    "\n",
    "# Load data\n",
    "data = load_and_prepare_data(DATA_PATH, normalize=True, verbose=False)\n",
    "\n",
    "# Get a sample\n",
    "sample_X = data['test_X'][:5]  # First 5 test samples\n",
    "sample_Y = data['test_Y'][:5]\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_X = sample_X.to(Config.DEVICE)\n",
    "    pred_scores = model(sample_X)\n",
    "    pred_placements = scores_to_placements(pred_scores)\n",
    "\n",
    "# Convert back to placements\n",
    "true_placements = relevance_to_placements(sample_Y)\n",
    "\n",
    "# Print results\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"Match {i+1}:\")\n",
    "    print(f\"  Predicted: {pred_placements[i].tolist()}\")\n",
    "    print(f\"  Actual:    {true_placements[i].tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Trained Model (for Colab)\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download best model (for Colab)\n",
    "# Uncomment if running on Colab\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('saved_models/best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook trains a TFT ranking model using:\n",
    "- **Architecture**: Multi-head self-attention (3 layers, 2 heads)\n",
    "- **Loss**: LambdaNDCG (optimized for ranking)\n",
    "- **Metrics**: NDCG@8, per-rank accuracy, MAE, Top-4 accuracy\n",
    "\n",
    "All hyperparameters are easily adjustable in Section 2.\n",
    "\n",
    "The trained model is saved to `saved_models/best_model.pt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
